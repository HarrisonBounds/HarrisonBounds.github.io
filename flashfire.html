<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Autonomous RC Car</title>
  <link rel="stylesheet" href="./style.css">
</head>
<body>
  <div class="other-container">
    <header>
      <h1>Flash Fire: Autonomous RC Car</h1>
      <br>
      <a style="color: #f39912;" href="https://github.com/HarrisonBounds/FlashFire" target="_blank" class="button-link">GitHub</a>

      <!-- Back to Homepage Button -->
      <a href="index.html" class="back-button">Back to Homepage</a>
    </header>

    <br>
    <section>
        <h2 style="color: #000000;">Overview</h2>
        <div class="other-video-container">
            <video class="other-square-video" autoplay muted loop width="400" height="300">
                <source src="imgs/flashfire_portfolio_shortened.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
          </div>
          <p style="text-align:center; font-style:italic; color:gray;">Successful Autonomous Run</p>
          
      <p>
        FlashFire is a group project that successfully produced an autonomous RC Car that could navigate any course using <b>Imitation Learning</b>.
        Imitation Learning works by having an "expert" perform the task a number of times and having the target (in this case an RC Car) learn from the expert.
        This was achieved by creating a custom <b>Convolutional Neural Network</b> to label each image being captured during training, and assigning that image
        a throttle value and steering value. 
    </section>

    <br>

    <section>
      <h2 style="color: #000000;">Hardware</h2>
      <!-- <div class="hardware-image"> <img src="imgs/franka_datasheet.png" alt="Franka Emika Panda Robot" style="width:100%; max-width:500px; display:block; margin:auto;"> <p style="text-align:center; font-style:italic; color:gray;">Franka Emika Panda Robot</p> </div> -->
      
      <p>
        The brain of this project was provided by the <b>Raspberry Pi 4</b>. Attached to the Pi was a webcam that captured 20 images per second so the 
        network had ample training data. Along with this, a servo was attached to <b>PCA9685</b> servo driver to control the front two wheels. A motor was also 
        attached to the PI, with everything being powered by a rechargeable LiPo battery. A custom 2-tier acrylic frame sat on top of the wheels that held all of
        the electronics in place.
    </p>
    </section>

    <br>

    <section>
      <h2 style="color: #000000;">Donkey Car</h2>
      <!-- <div class="processed-image"> <img src="imgs/yanni_doodle_droid.png" alt="Yanni Draws Himself" style="width:100%; max-width:300px; display:block; margin:auto;"> <p style="text-align:center; font-style:italic; color:gray;">Yanni Draws Himself</p> </div> -->
      
      <p>
        This project was built off a platform known as <b>Donkey Car</b>. <a href="https://github.com/autorope/donkeycar" target="_blank">DonkeyCar</a> is an open-source platform for building self-driving scale model cars, designed as an accessible entry point to autonomous 
        vehicle development. It combines hardware, such as a 1/10th scale RC car, Raspberry Pi, and various sensors, with Python-based software 
        to enable machine learning and computer vision tasks. DonkeyCar utilizes deep learning frameworks like TensorFlow and PyTorch to train models 
        for lane following, object avoidance, and more.
        <br>
        <br>
        However for this project, we deployed our own Convolutional Neural Network to fit the specific needs of this project using <b>PyTorch</b>. 
    </p>
    </section>

    <br>

    <section>
        <h2 style="color: #000000;">Architecture</h2>
        <!-- <div class="april-tag"> <img src="imgs/doodle-droid_april_tag.png" alt="April Tag" style="width:100%; max-width:300px; display:block; margin:auto;"> <p style="text-align:center; font-style:italic; color:gray;">April Tag</p> </div> -->
        
        <p>
            Flashfire's network is a CNN architecture designed for tasks involving image input, characterized by its compact 
            and modular design. It begins with four convolutional layers that progressively reduce the spatial dimensions of the input while 
            increasing feature depth, employing 5x5 and 3x3 kernels with strides of 2 and 1 to capture patterns at varying scales. Each convolutional 
            layer is followed by a ReLU activation to introduce non-linearity. After the convolutional operations, the network flattens the feature 
            maps and processes them through three fully connected layers, reducing the dimensionality step-by-step from a computed input size to 64, 
            then 32, and finally to 2 output neurons (throttle and steering). This architecture ensures that the network is adaptable to different input sizes (defined by 
            width and height) by dynamically calculating the fully connected input size based on convolutional layer outputs.
      </p>
      </section>

      <br>

    <section>
      <h2 style="color: #000000;">Results</h2>
      <!-- <div class="image-container">
        <div class="route-image">
          <img src="imgs/doodle_droid_image_view.png" alt="Output" style="width:100%; max-width:300px; display:block; margin:auto;">
          <p style="text-align:center; font-style:italic; color:gray;">Processed Image</p>
        </div>
        <div class="extra-image">
          <img src="imgs/doodle_droid_output.png" alt="Second Image" style="width:100%; max-width:300px; display:block; margin:auto;">
          <p style="text-align:center; font-style:italic; color:gray;">Route Planner Output from <i>/plot</i> service</p>
        </div>
      </div> -->
      <p>
        After moving the training images from the PI to a hose computer, FlashFire is ready to be trained for an autnomous run on the course driven on.
        Running the images through FlashFire's CNN produces a model that is used as an input to the autopilot script. The script takes an image similar to 
        the training process, runs that image through the model, and outputs a throttle and steering value for the car. 
      </p>
    </section>

    <br>

    <!-- Contributors Section -->
    <section id="contributors">
      <h2 style="color: #000000;">Contributors</h2>
      <ul>
        <li>Brandon Donaldson</li>
        <li>Lawan</li>
        <li><a href="https://github.com/linzhangUCA" target="_blank">Lin Zhang</a></li>
      </ul>
    </section>
  </div>
</body>
</html>
