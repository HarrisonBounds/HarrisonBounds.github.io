<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Doodle Droid Project</title>
  <link rel="stylesheet" href="./style.css">
</head>
<body>
  <div class="other-container">
    <header>
      <h1>Doodle Droid</h1>
      <br>
      <a style="color: #f39912;" href="https://github.com/HarrisonBounds/DoodleDroid" target="_blank" class="button-link">GitHub</a>

      <!-- Back to Homepage Button -->
      <a href="index.html" class="back-button">Back to Homepage</a>
    </header>

    <br>
    <section>
        <h2 style="color: #000000;">Overview</h2>
        <div class="other-video-container">
            <video class="other-square-video" autoplay muted loop width="400" height="300">
                <source src="imgs/luffy_doodle_droid_compressed.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
          </div>
          <p style="text-align:center; font-style:italic; color:gray;">Drawing from File</p>
          <br>
      <p>
        Doodle Droid is a 7 DoF arm that detects an image from a camera and processes that image into a drawable format. This project leverages ROS 2 to have 
        an architecture that can pass messages between nodes with ease. This was a <b>group effort</b>, where the work was split up into different nodes. These
        nodes consisted of image processing, calibration, route planning, and motion planning. My main task was image processing, but each member in our group helped
        with several aspects of the project. These tasks are explained below.
    </section>

    <section>
      <h2 style="color: #000000;">Hardware</h2>
      <div class="hardware-image"> <img src="imgs/franka_datasheet.png" alt="Franka Emika Panda Robot" style="width:100%; max-width:500px; display:block; margin:auto;"> <p style="text-align:center; font-style:italic; color:gray;">Franka Emika Panda Robot</p> </div>
      <br>
      <p>
        The Franka Emika Panda is a versatile robotic arm known for its precision and user-friendly design. 
        With seven joints, it moves like a human arm, making it great for tasks that require dexterity. 
        Its built-in torque sensors let it handle delicate jobs safely and accurately. 
        Lightweight and compact, the Panda is easy to integrate into projects, whether it’s for research, automation, or interactive applications. 
        It’s also compatible with ROS 2, making it a go-to choice for modern robotics development.      </p>
    </section>

    <section>
      <h2 style="color: #000000;">Image Processing</h2>
      <div class="processed-image"> <img src="imgs/yanni_doodle_droid.png" alt="Yanni Draws Himself" style="width:100%; max-width:300px; display:block; margin:auto;"> <p style="text-align:center; font-style:italic; color:gray;">Yanni Draws Himself</p> </div>
      <br>
      <p>
        The usb_cam_node allows the compute webcam publish images over a topic for other nodes to use.
        Starting this node allows for a simple picture taking service, which takes the last image sent on the <i>/images</i>
        topic and uses that image. Doodle Droid is also capable of drawing images straight from file, as long as its in the correct directory.
        After receiving the image, an external repo is used to process the collected image into a line drawing.
        A link for this GitHub repository can be found <a href="https://github.com/LingDong-/linedraw" target="_blank">here</a>.
        In a nutshell, this code converts the pixels into strokes, allowing for a real-time image to become a drawing.
    </p>
    </section>

    <section>
        <h2 style="color: #000000;">Calibration</h2>
        <div class="april-tag"> <img src="imgs/doodle-droid_april_tag.png" alt="April Tag" style="width:100%; max-width:300px; display:block; margin:auto;"> <p style="text-align:center; font-style:italic; color:gray;">April Tag</p> </div>
        <br>
        <p>
          The calibration was used to get the <b>z height</b> so the arm knew how much to lower itself. To achieve this, our workspace had four of the April tags
          above on the edges of the paper. These position were then averaged to get the center of the paper. With this information, we had the option to 
          adjust our paper size however we saw fit. Of course drawing smaller images took a shorter time, but larger images produced more detail. To detect the april tags,
          we utilized a realsense depth camera by placing the end effector in a position above all of the tags. By detecting the tags this way, we could draw our
          image at any orientation, as long as it remained in the franka's workspace
      </p>
      </section>

    <section>
      <h2 style="color: #000000;">Route Planning</h2>
      <div class="image-container">
        <div class="route-image">
          <img src="imgs/doodle_droid_image_view.png" alt="Output" style="width:100%; max-width:300px; display:block; margin:auto;">
          <p style="text-align:center; font-style:italic; color:gray;">Processed Image</p>
        </div>
        <!-- New image beside the Route Planner image -->
        <div class="extra-image">
          <img src="imgs/doodle_droid_output.png" alt="Second Image" style="width:100%; max-width:300px; display:block; margin:auto;">
          <p style="text-align:center; font-style:italic; color:gray;">Route Planner Output from <i>/plot</i> service</p>
        </div>
      </div>
      <br>
      <p>
        The route planner node receives the "waypoints", produced from the image processor. From here, it normalizes these points to the size of our workspace.
        This line data is further converted into 3D waypoints for the robot to follow. This node provides two services. The <i>plot</i> service
        visualizes the planned path for the user. The <i>draw</i> service starts the drawing process of the processed image. The route planner sends these 
        waypoints along to the motion planning node in addition to intermediate commands to pick up and put down the drawing utensil.
      </p>
    </section>

    <section>
      <h2 style="color: #000000;">Motion Planning</h2>
      <div class="other-video-container">
        <video class="other-square-video" autoplay muted loop width="400" height="300">
            <source src="imgs/harrison_doodle_droid.MOV" type="video/mp4">
            Your browser does not support the video tag.
        </video>
      </div>
      <p style="text-align:center; font-style:italic; color:gray;">Drawing from Live Image</p>
      <br>
      <p>
        Motion planning is implemented using ROS MoveIt! API, ensuring smooth and precise movements of the robotic arm. It can plan several different paths,
        including cartesian, which is the most useful for drawing, since our workspace is constricted to a 2D plane (excluding picking up the drawing utensil).
        There is also ample error handling within the motion planning node for debugging purposes. Along with each of the joints on the Franka Arm, the motion 
        planner also controls the gripper state, which is not required for this particular task. 
      </p>
    </section>

    <!-- Contributors Section -->
    <section id="contributors">
      <h2 style="color: #000000;">Contributors</h2>
      <p>Project contributors:</p>
      <ul>
        <li><a href="https://github.com/CappuccinoPanda" target="_blank">David Matthews</a></li>
        <li><a href="https://github.com/0nhc " target="_blank">Zhengxiao Han</a></li>
        <li><a href="https://github.com/Ykechriotis" target="_blank">Yanni Kechriotis</a><li>
        <li>Christian Tongate</li>
        <!-- Add more contributors here -->
      </ul>
    </section>
  </div>
</body>
</html>
